{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 1.6413 - accuracy: 0.5604 - val_loss: 0.8180 - val_accuracy: 0.8107\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.6144 - accuracy: 0.8380 - val_loss: 0.4514 - val_accuracy: 0.8742\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.4315 - accuracy: 0.8794 - val_loss: 0.3646 - val_accuracy: 0.8968\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.3670 - accuracy: 0.8964 - val_loss: 0.3243 - val_accuracy: 0.9075\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.3314 - accuracy: 0.9059 - val_loss: 0.2981 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.3062 - accuracy: 0.9121 - val_loss: 0.2774 - val_accuracy: 0.9209\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2869 - accuracy: 0.9171 - val_loss: 0.2638 - val_accuracy: 0.9254\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.2701 - accuracy: 0.9223 - val_loss: 0.2497 - val_accuracy: 0.9286\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2564 - accuracy: 0.9266 - val_loss: 0.2398 - val_accuracy: 0.9311\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2434 - accuracy: 0.9294 - val_loss: 0.2301 - val_accuracy: 0.9339\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2322 - accuracy: 0.9327 - val_loss: 0.2196 - val_accuracy: 0.9365\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2214 - accuracy: 0.9356 - val_loss: 0.2142 - val_accuracy: 0.9395\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2115 - accuracy: 0.9386 - val_loss: 0.2026 - val_accuracy: 0.9430\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2024 - accuracy: 0.9413 - val_loss: 0.1979 - val_accuracy: 0.9444\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.1936 - accuracy: 0.9443 - val_loss: 0.1931 - val_accuracy: 0.9443\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1859 - accuracy: 0.9463 - val_loss: 0.1847 - val_accuracy: 0.9483\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1783 - accuracy: 0.9482 - val_loss: 0.1777 - val_accuracy: 0.9494\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1715 - accuracy: 0.9505 - val_loss: 0.1720 - val_accuracy: 0.9522\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1648 - accuracy: 0.9521 - val_loss: 0.1678 - val_accuracy: 0.9531\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1588 - accuracy: 0.9541 - val_loss: 0.1624 - val_accuracy: 0.9548\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "Test score: 0.16248419483676552\n",
      "Test accuracy: 0.95169997215271\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784 \n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "        optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 1.4258 - accuracy: 0.6515 - val_loss: 0.7145 - val_accuracy: 0.8439\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.5821 - accuracy: 0.8549 - val_loss: 0.4473 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.4336 - accuracy: 0.8842 - val_loss: 0.3698 - val_accuracy: 0.8988\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3752 - accuracy: 0.8960 - val_loss: 0.3336 - val_accuracy: 0.9061\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3420 - accuracy: 0.9046 - val_loss: 0.3084 - val_accuracy: 0.9146\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.3191 - accuracy: 0.9109 - val_loss: 0.2922 - val_accuracy: 0.9191\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.3013 - accuracy: 0.9157 - val_loss: 0.2770 - val_accuracy: 0.9231\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2866 - accuracy: 0.9199 - val_loss: 0.2650 - val_accuracy: 0.9285\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.2746 - accuracy: 0.9222 - val_loss: 0.2552 - val_accuracy: 0.9300\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2635 - accuracy: 0.9264 - val_loss: 0.2476 - val_accuracy: 0.9318\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2536 - accuracy: 0.9289 - val_loss: 0.2384 - val_accuracy: 0.9339\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2444 - accuracy: 0.9315 - val_loss: 0.2312 - val_accuracy: 0.9366\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.2361 - accuracy: 0.9343 - val_loss: 0.2270 - val_accuracy: 0.9364\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2287 - accuracy: 0.9362 - val_loss: 0.2196 - val_accuracy: 0.9386\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.2214 - accuracy: 0.9382 - val_loss: 0.2125 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2147 - accuracy: 0.9397 - val_loss: 0.2069 - val_accuracy: 0.9423\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2085 - accuracy: 0.9412 - val_loss: 0.2024 - val_accuracy: 0.9439\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.2025 - accuracy: 0.9424 - val_loss: 0.1973 - val_accuracy: 0.9451\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1968 - accuracy: 0.9445 - val_loss: 0.1939 - val_accuracy: 0.9457\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.1918 - accuracy: 0.9458 - val_loss: 0.1891 - val_accuracy: 0.9470\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "Test score: 0.18930720813423396\n",
      "Test accuracy: 0.9456999897956848\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784 \n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "        optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 1.3052 - accuracy: 0.6826 - val_loss: 0.7242 - val_accuracy: 0.8494\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.6205 - accuracy: 0.8516 - val_loss: 0.4933 - val_accuracy: 0.8809\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.4839 - accuracy: 0.8746 - val_loss: 0.4165 - val_accuracy: 0.8916\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.4240 - accuracy: 0.8857 - val_loss: 0.3762 - val_accuracy: 0.8980\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.3884 - accuracy: 0.8939 - val_loss: 0.3500 - val_accuracy: 0.9054\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.3638 - accuracy: 0.8993 - val_loss: 0.3319 - val_accuracy: 0.9086\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3452 - accuracy: 0.9038 - val_loss: 0.3165 - val_accuracy: 0.9113\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3301 - accuracy: 0.9082 - val_loss: 0.3050 - val_accuracy: 0.9152\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3175 - accuracy: 0.9116 - val_loss: 0.2949 - val_accuracy: 0.9180\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.3067 - accuracy: 0.9146 - val_loss: 0.2861 - val_accuracy: 0.9196\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.2970 - accuracy: 0.9171 - val_loss: 0.2778 - val_accuracy: 0.9219\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.2880 - accuracy: 0.9197 - val_loss: 0.2708 - val_accuracy: 0.9243\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.2802 - accuracy: 0.9226 - val_loss: 0.2652 - val_accuracy: 0.9257\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.2730 - accuracy: 0.9241 - val_loss: 0.2582 - val_accuracy: 0.9282\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2662 - accuracy: 0.9256 - val_loss: 0.2530 - val_accuracy: 0.9294\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.2598 - accuracy: 0.9275 - val_loss: 0.2475 - val_accuracy: 0.9321\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2540 - accuracy: 0.9293 - val_loss: 0.2427 - val_accuracy: 0.9330\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2485 - accuracy: 0.9305 - val_loss: 0.2384 - val_accuracy: 0.9332\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2432 - accuracy: 0.9321 - val_loss: 0.2339 - val_accuracy: 0.9357\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.2384 - accuracy: 0.9337 - val_loss: 0.2299 - val_accuracy: 0.9362\n",
      "10000/10000 [==============================] - 0s 39us/step\n",
      "Test score: 0.23213751099705696\n",
      "Test accuracy: 0.9355999827384949\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784 \n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "        optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the results of using two hidden layers versus three or one,  I have concluded that there is a consistent trend between these parameter changes; particularly in regard to training and validation accuracy rates. Increasing the amount of hidden layers initially causes a rise in both training and validation accuracy, due to the model learning complex patterns in the data; the downside is, if the added layers are too complex, the model may begin to \"overfit\" the training data, negatively impacting validation accuracy. Overfitting is essentially when the ML model aims to memorize its training without weighing in newly introduced or unseen data. For this reason, test accuracy may also decrease. On the other hand, reducing the number of hidden layers appers to cause a decrease in both training and validation, as well as test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
